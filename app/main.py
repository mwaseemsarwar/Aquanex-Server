from __future__ import annotations

import asyncio
import orjson
import structlog
from fastapi import FastAPI, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from .config import settings
from .logging_conf import setup_logging
from .middlewares import log_requests
from .schemas import ChatRequest, ErrorResponse
from .utils import includes_any, INFORMAL_PATTERNS, ALLOWED_TOPICS
from .llm_service import stream_openai, stream_fallback
from .redis_client import get_redis, cache_key_from_prompt

# ---------------------------------------------------
# Setup logging & app
# ---------------------------------------------------
setup_logging()
logger = structlog.get_logger()

app = FastAPI(title=settings.APP_NAME)

# ---------------------------------------------------
# CORS
# ---------------------------------------------------
origins = [o.strip() for o in settings.CORS_ORIGINS.split(",") if o.strip()]
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"] if origins == ["*"] else origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.middleware("http")(log_requests)


# ---------------------------------------------------
# Routes
# ---------------------------------------------------
@app.get("/")
async def root():
    """Root metadata endpoint."""
    return {
        "service": settings.APP_NAME,
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "chat": "/chat (POST)",
            "health": "/health (GET)",
        },
    }


@app.get("/health")
async def health():
    """Simple health check."""
    return {"status": "ok"}


@app.post("/chat", responses={500: {"model": ErrorResponse}})
async def chat(payload: ChatRequest):
    """
    Main chat endpoint:
    - Checks if message is informal/allowed
    - Uses Redis caching if available
    - Streams from OpenAI or fallback generator
    """
    try:
        messages = [m.model_dump() for m in payload.messages]
        selected_model = payload.selectedModel

        latest = (messages[-1]["content"] or "").lower() if messages else ""
        is_informal = includes_any(latest, INFORMAL_PATTERNS)
        is_allowed = includes_any(latest, ALLOWED_TOPICS)

        r = await get_redis()
        cache_key = cache_key_from_prompt(latest, selected_model or settings.OPENAI_MODEL)

        async def stream_result(gen):
            """Helper: stream tokens & cache final output if Redis available."""
            full: list[str] = []
            try:
                async for chunk in gen:
                    full.append(chunk)
                    yield chunk
            finally:
                if r is not None and full:
                    try:
                        await r.set(cache_key, "".join(full), ex=60 * 30)
                    except Exception as e:
                        logger.warning("redis_set_failed", error=str(e))

        # Restrict content (if not allowed â†’ fallback model)
        if not (is_informal or is_allowed):
            return StreamingResponse(
                stream_result(stream_fallback()),
                media_type="text/plain; charset=utf-8",
            )

        # Redis cache lookup
        if r is not None:
            try:
                cached = await r.get(cache_key)
                if cached:
                    async def stream_cached():
                        text = cached.decode("utf-8") if isinstance(cached, bytes) else cached  
                        for ch in text:
                            yield ch
                            await asyncio.sleep(0)

                    return StreamingResponse(
                        stream_cached(),
                        media_type="text/plain; charset=utf-8",
                    )
            except Exception as e:
                logger.warning("redis_get_failed", error=str(e))

        # Fallback: stream from OpenAI
        return StreamingResponse(
            stream_result(stream_openai(messages, selected_model)),
            media_type="text/plain; charset=utf-8",
        )

    except ValueError as e:
        logger.warning("validation_error", error=str(e))
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content=ErrorResponse(error=str(e)).model_dump(),
        )
    except PermissionError as e:
        logger.error("openai_auth_or_quota_error", error=str(e))
        return JSONResponse(
            status_code=status.HTTP_403_FORBIDDEN,
            content=ErrorResponse(error=str(e)).model_dump(),
        )
    except Exception as e:
        logger.exception("chat_error", error=str(e))
        return JSONResponse(
            status_code=500,
            content=ErrorResponse(error="Internal server error").model_dump(),
        )
